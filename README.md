# 原来的爬虫demo
刚学的时候写的，都不太好。今天清理电脑准备删除，留个备份以后可以参考一下

有时间再整理一下吧，这回属于无脑上传

环境和邮编使用scrapy，微博使用scrapy+selenium(无界面自行添加phantomjs或其它)
## 爬取环境信息
- 结构较清晰吧，爬取环境信息的
## 爬取微博
- 新浪微博搜索关键字
    - 因为使用phantomjs进行无界面登陆过程非常繁琐麻烦，所以暂时只使用seleniumn
    - 直接使用用户名密码进行填充登陆
    - 登陆后填充需要的关键词，然后进行搜索
    - 可以爬取50页内容，将用户名、用户头像、正文、点赞数、转发数、评论数、发布时间、发布所用设备保存
- 特别处理
    - 正文有点开全文的，无法直接抓取，有一层js，分析界面后采用得到动作链接后进行拼接发送请求的方法获得
    - 许多内容判空需要trycatch操作
    - 时间内容格式化
- 无界面
	- 使用phantomjs或者chromedrive有设置项
## 爬取邮编
- Linux和windows上编码不一样，注释里注明方法
- 需要解决的问题已解决，原来需要进行的数据清洗操作已不用进行
- 爬虫执行完毕后，需要清楚数据库中 place 为空的地点
    - [示例网页](http://www.yb21.cn/post/code/065201.html),最后一个元素为空但是标签
    - 直接清洗数据库数据即可，再次爬取时可以修改爬虫。本次由于已经爬了两小时了，不再修改
    - 清洗数据库命令: 切换到相关数据库下，执行: `db.getCollection('zipCode').remove({"place":""})`
- 部分地区地址最终页数据有以下三种异常情况，需要处理(已处理)
    - 城内所有街道及单位
    - 全县各乡镇(N全县各乡镇#)
    - 其余各乡镇(N其余各乡镇#)
    
- `db.getCollection('zipCode').find({"place":{"$regex":"各乡镇|所有街道"}})`
- 查询异常数据
````
db.getCollection('zipCode').aggregate([
    { $match : { place : { $regex:"各乡镇|所有街道" } } },
    {$group: {_id: "$place", num_tutorial: {$sum: 1
            }
        }
    },
    { $match : { num_tutorial : { $gt:0 } } },
])
````
# ETC
etc中的不是爬虫，是通过分析页面找到请求直接获取信息，traffic为一个获取demo，heat为了记录加入header和cookie的样例